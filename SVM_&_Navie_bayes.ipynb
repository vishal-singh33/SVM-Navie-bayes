{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 :  What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "Answer:-Information Gain is a key concept in machine learning, particularly in Decision Tree algorithms such as ID3, C4.5, and CART. It is a measure used to select the best attribute (feature) that splits the dataset into the most homogeneous subsets. The goal of a decision tree is to create branches that lead to pure or nearly pure class labels.\n",
        "Information Gain helps determine which feature provides the most useful information about the target variable.\n",
        "\n",
        "2. Basic Idea\n",
        "\n",
        "When building a Decision Tree, we start with the entire dataset (called the root node).\n",
        "We must decide which attribute to split on first to best separate the data into classes.\n",
        "Information Gain tells us how much “information” (reduction in uncertainty or impurity) is achieved if we split the data using a specific attribute.\n",
        "\n",
        "In other words:\n",
        "\n",
        "Information Gain = Reduction in Entropy after the dataset is split on an attribute.\n",
        "\n",
        "3. Key Concepts Used in Information Gain\n",
        "(a) Entropy\n",
        "\n",
        "Entropy is a measure of impurity or randomness in the dataset.\n",
        "It indicates how mixed the data is in terms of class labels.\n",
        "\n",
        "For a binary classification problem, entropy is calculated as:\n",
        "\n",
        "Entropy\n",
        "(\n",
        "𝑆\n",
        ")\n",
        "=\n",
        "−\n",
        "𝑝\n",
        "1\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "1\n",
        ")\n",
        "−\n",
        "𝑝\n",
        "2\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "2\n",
        ")\n",
        "Entropy(S)=−p\n",
        "1\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "(p\n",
        "1\n",
        "\t​\n",
        "\n",
        ")−p\n",
        "2\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "(p\n",
        "2\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "1\n",
        "p\n",
        "1\n",
        "\t​\n",
        "\n",
        " = proportion of positive examples\n",
        "\n",
        "𝑝\n",
        "2\n",
        "p\n",
        "2\n",
        "\t​\n",
        "\n",
        " = proportion of negative examples\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "If all samples belong to one class (pure node), Entropy = 0\n",
        "\n",
        "If samples are evenly split (50%-50%), Entropy = 1 (maximum impurity)\n",
        "\n",
        "(b) Information Gain Formula\n",
        "Information Gain (IG)\n",
        "=\n",
        "Entropy (Parent)\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑘\n",
        "∣\n",
        "𝑆\n",
        "𝑖\n",
        "∣\n",
        "∣\n",
        "𝑆\n",
        "∣\n",
        "×\n",
        "Entropy\n",
        "(\n",
        "𝑆\n",
        "𝑖\n",
        ")\n",
        "Information Gain (IG)=Entropy (Parent)−\n",
        "i=1\n",
        "∑\n",
        "k\n",
        "\t​\n",
        "\n",
        "∣S∣\n",
        "∣S\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣\n",
        "\t​\n",
        "\n",
        "×Entropy(S\n",
        "i\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑆\n",
        "S = parent dataset before splitting\n",
        "\n",
        "𝑆\n",
        "𝑖\n",
        "S\n",
        "i\n",
        "\t​\n",
        "\n",
        " = subset of data after splitting by attribute\n",
        "\n",
        "∣\n",
        "𝑆\n",
        "𝑖\n",
        "∣\n",
        "/\n",
        "∣\n",
        "𝑆\n",
        "∣\n",
        "∣S\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣/∣S∣ = proportion of subset\n",
        "𝑖\n",
        "i\n",
        "\n",
        "𝑘\n",
        "k = number of possible values (branches) of the attribut\n",
        "\n",
        "5. Role in Decision Trees\n",
        "\n",
        "Information Gain is used to:\n",
        "\n",
        "Select the best feature to split the dataset at each node.\n",
        "\n",
        "Grow the Decision Tree recursively, choosing the feature with the highest Information Gain at each step.\n",
        "\n",
        "Stop splitting when Information Gain becomes zero or below a threshold (indicating pure subsets).\n",
        "\n",
        "This process continues until:\n",
        "\n",
        "All samples are classified perfectly, or\n",
        "\n",
        "The tree reaches a predefined depth.\n",
        "\n",
        "6. Advantages of Using Information Gain\n",
        "\n",
        "Intuitive and mathematically sound measure.\n",
        "\n",
        "Helps create smaller and more efficient trees.\n",
        "\n",
        "Works well with categorical data.\n",
        "\n",
        "Reduces impurity and improves prediction accuracy.\n",
        "\n",
        "7. Limitations\n",
        "\n",
        "Information Gain favors attributes with many distinct values (like ID numbers).\n",
        "\n",
        "It can lead to overfitting if not regularized.\n",
        "\n",
        "For continuous features, proper binning or thresholding is required.\n",
        "\n",
        "8. Alternatives\n",
        "\n",
        "To overcome the bias of Information Gain, C4.5 algorithm introduced Gain Ratio, which normalizes Information Gain by the “Split Information.”\n",
        "\n",
        "Gain Ratio\n",
        "=\n",
        "Information Gain\n",
        "Split Information\n",
        "Gain Ratio=\n",
        "Split Information\n",
        "Information Gain\n",
        "\t​\n",
        "\n",
        "\n",
        "This balances attribute selection more fairly.\n",
        "\n",
        "9. Conclusion\n",
        "\n",
        "In summary:\n",
        "\n",
        "Information Gain measures how much uncertainty is reduced by splitting the data using a given feature.\n",
        "\n",
        "It plays a central role in constructing Decision Trees, ensuring that each decision (split) maximally increases the “purity” of the subsets.\n",
        "\n",
        "By repeatedly applying Information Gain, we grow an interpretable and effective Decision Tree model."
      ],
      "metadata": {
        "id": "NnEp-_5OQF-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2: What is the difference between Gini Impurity and Entropy?**\n",
        "**Hint: Directly compares the two main impurity measures, highlighting strengths,**\n",
        "**weaknesses, and appropriate use cases.**\n",
        "\n",
        "Answer:-Introduction\n",
        "\n",
        "In Decision Tree algorithms, such as CART, ID3, and C4.5, the goal is to split the data in a way that creates pure subsets — that is, subsets where most (or all) data points belong to one class.\n",
        "To measure this impurity or disorder, two popular metrics are used: Entropy and Gini Impurity.\n",
        "\n",
        "Both are used to decide which attribute to split on at each step while building a decision tree, but they differ slightly in how they calculate impurity and how sensitive they are to class distributions.\n",
        "\n",
        "Definitions\n",
        "(a) Entropy\n",
        "\n",
        "Entropy measures the amount of randomness or impurity in a dataset.\n",
        "It is based on information theory (Shannon’s concept of Information Entropy).\n",
        "\n",
        "Entropy\n",
        "(\n",
        "𝑆\n",
        ")\n",
        "=\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "log\n",
        "⁡\n",
        "2\n",
        "(\n",
        "𝑝\n",
        "𝑖\n",
        ")\n",
        "Entropy(S)=−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        "log\n",
        "2\n",
        "\t​\n",
        "\n",
        "(p\n",
        "i\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        " = proportion of class\n",
        "𝑖\n",
        "i in dataset\n",
        "𝑆\n",
        "S\n",
        "\n",
        "𝑐\n",
        "c = number of classes\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Entropy = 0 → data is perfectly pure (all in one class)\n",
        "\n",
        "Entropy = 1 → data is maximally impure (equal class distribution)\n",
        "\n",
        "(b) Gini Impurity\n",
        "\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the class distribution in the dataset.\n",
        "\n",
        "Gini\n",
        "(\n",
        "𝑆\n",
        ")\n",
        "=\n",
        "1\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "=\n",
        "1\n",
        "𝑐\n",
        "𝑝\n",
        "𝑖\n",
        "2\n",
        "Gini(S)=1−\n",
        "i=1\n",
        "∑\n",
        "c\n",
        "\t​\n",
        "\n",
        "p\n",
        "i\n",
        "2\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑝\n",
        "𝑖\n",
        "p\n",
        "i\n",
        "\t​\n",
        "\n",
        " = proportion of class\n",
        "𝑖\n",
        "i in dataset\n",
        "𝑆\n",
        "S\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "Gini = 0 → completely pure node\n",
        "\n",
        "Gini = maximum (≈0.5 for binary) → most impure split (equal mix of classes)\n",
        "\n",
        "Graphical Relationship\n",
        "\n",
        "If we plot both measures (Entropy vs Gini) for different class probabilities:\n",
        "\n",
        "Both start at 0 (pure node) when one class probability = 1.\n",
        "\n",
        "Both reach a maximum at 50-50 distribution.\n",
        "\n",
        "The curves are similar, but Gini is slightly more sensitive near pure nodes (responds more to small changes in class probability).\n",
        "\n",
        "6. Strengths and Weaknesses\n",
        " Entropy – Strengths\n",
        "\n",
        "Rooted in information theory, gives a strong theoretical foundation.\n",
        "\n",
        "More accurate in representing uncertainty for complex distributions.\n",
        "\n",
        "Good when interpretability and mathematical meaning matter.\n",
        "\n",
        " Entropy – Weaknesses\n",
        "\n",
        "Slightly slower to compute (uses logarithms).\n",
        "\n",
        "May be harder to interpret intuitively compared to Gini.\n",
        "\n",
        " Gini Impurity – Strengths\n",
        "\n",
        "Computationally faster (no logarithms).\n",
        "\n",
        "Works well in practice with large datasets.\n",
        "\n",
        "Produces similar trees to Entropy with less computation.\n",
        "\n",
        " Gini Impurity – Weaknesses\n",
        "\n",
        "Slightly less sensitive to changes in class probability.\n",
        "\n",
        "Sometimes biased toward attributes with more unique values.\n",
        "\n",
        "Relationship Between the Two\n",
        "\n",
        "Both aim to minimize impurity and maximize purity at each node.\n",
        "\n",
        "Often, both produce very similar decision trees in practice.\n",
        "\n",
        "Gini and Entropy differ mostly in scale and sensitivity, not outc\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Both Gini Impurity and Entropy are impurity measures used to decide how to split data in a Decision Tree.\n",
        "\n",
        "Entropy measures information content — how unpredictable the dataset is.\n",
        "\n",
        "Gini Impurity measures misclassification probability.\n",
        "\n",
        "While Entropy is theoretically grounded, Gini is computationally faster.\n",
        "In most real-world applications, both give similar results, and the choice depends on algorithm type (CART vs ID3) or computational efficiency needs."
      ],
      "metadata": {
        "id": "x5UjTDPOQ1ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "Answer:- Introduction\n",
        "\n",
        "Decision Trees are powerful and interpretable machine-learning models that split data into smaller subsets until they reach the most homogeneous (pure) class labels.\n",
        "However, if we allow a tree to grow without any restriction, it may become too complex — perfectly fitting the training data but performing poorly on unseen data.\n",
        "This problem is called overfitting.\n",
        "\n",
        "To overcome overfitting, pruning techniques are used.\n",
        "Pruning can be of two types:\n",
        "\n",
        "Pre-Pruning (Early Stopping)\n",
        "\n",
        "Post-Pruning (Pruning after Tree Construction)\n",
        "\n",
        "This answer focuses on Pre-Pruning.\n",
        "\n",
        "Definition of Pre-Pruning\n",
        "\n",
        "Pre-Pruning, also known as Early Stopping, is a method where the growth of the decision tree is stopped early during its construction—before it perfectly classifies all the training data.\n",
        "\n",
        "It involves setting certain stopping criteria that prevent the algorithm from creating additional branches if further splitting does not significantly improve the model.\n",
        "\n",
        "Purpose of Pre-Pruning\n",
        "\n",
        "The main purpose is to:\n",
        "\n",
        "Avoid overfitting the training data.\n",
        "\n",
        "Reduce model complexity by controlling the depth or number of splits.\n",
        "\n",
        "Improve generalization performance on unseen data.\n",
        "\n",
        "Save computational time and memory\n",
        "\n",
        "How Pre-Pruning Works\n",
        "\n",
        "During the construction of a decision tree:\n",
        "\n",
        "The algorithm evaluates all possible splits for the current node.\n",
        "\n",
        "It checks if a potential split meets predefined stopping conditions.\n",
        "\n",
        "If the condition is not satisfied, the algorithm stops splitting that node, turning it into a leaf node.\n",
        "\n",
        "This means the tree is built only until it reaches the most useful and significant splits.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we are building a decision tree to classify whether a customer will buy a product.\n",
        "\n",
        "If we allow unlimited depth, the tree might memorize each customer (overfitting).\n",
        "\n",
        "Using Pre-Pruning, we can set:\n",
        "\n",
        "max_depth = 4\n",
        "\n",
        "min_samples_split = 10\n",
        "\n",
        "min_impurity_decrease = 0.01\n",
        "\n",
        "This ensures that the tree stops growing once it stops learning meaningful patterns, creating a simpler and more generalizable model.\n",
        "\n",
        "Advantages of Pre-Pruning\n",
        "\n",
        "Prevents overfitting by controlling model complexity.\n",
        " Produces smaller and faster trees.\n",
        " Reduces training time and memory usage.\n",
        " Often leads to better performance on unseen test data.\n",
        "\n",
        " Disadvantages of Pre-Pruning\n",
        "\n",
        " May cause underfitting if stopped too early (the tree might miss important patterns).\n",
        " Choosing the optimal stopping parameters can be difficult and dataset-dependent.\n",
        " Might ignore useful splits that could improve accuracy later.\n",
        "\n",
        " When to Use Pre-Pruning\n",
        "\n",
        "Use Pre-Pruning when:\n",
        "\n",
        "You have limited data and want to avoid overfitting.\n",
        "\n",
        "You need a quick, interpretable model.\n",
        "\n",
        "You are working with large datasets and want to reduce training cost.\n",
        "\n",
        "The application demands real-time or resource-efficient tree construction.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "Pre-Pruning is an early-stopping technique used in Decision Trees to prevent overfitting by halting the tree-building process when further splitting provides little to no benefit.\n",
        "It makes the model simpler, faster, and more generalizable, though at the risk of underfitting if used too aggressively."
      ],
      "metadata": {
        "id": "XTNXsod-Rx0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:Write a Python program to train a Decision Tree Classifier using Gini**\n",
        "**Impurity as the criterion and print the feature importances (practical).**\n",
        "**Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.**\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:-Aim\n",
        "\n",
        "To build and train a Decision Tree Classifier using the Gini Impurity criterion, and display the importance of each feature that contributed to the model’s decision-making.\n",
        "\n",
        "Algorithm / Steps\n",
        "\n",
        "Import necessary libraries (pandas, sklearn).\n",
        "\n",
        "Load a sample dataset (e.g., Iris dataset).\n",
        "\n",
        "Split the data into training and testing sets.\n",
        "\n",
        "Create a Decision Tree Classifier using criterion='gini'.\n",
        "\n",
        "Train (fit) the model on the training data.\n",
        "\n",
        "Predict results on the test data.\n",
        "\n",
        "Print model accuracy and feature importances.\n",
        "\n",
        "\n",
        "Model Accuracy: 100.0 %\n",
        "\n",
        "Feature Importances:\n",
        "sepal length (cm): 0.0\n",
        "sepal width (cm): 0.027\n",
        "petal length (cm): 0.545\n",
        "petal width (cm): 0.428\n",
        "\n",
        "\n",
        "Conclusion\n",
        "\n",
        "This program demonstrates how to build and train a Decision Tree Classifier using Gini Impurity in Python.\n",
        "The .feature_importances_ attribute provides valuable insights into which features are most influential for classification."
      ],
      "metadata": {
        "id": "H7JnUuQbSsWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Step 4: Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {round(importance, 3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5huEId-PTyJA",
        "outputId": "fb4c0750-00f4-446b-8c52-2f1157f7292b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.0 %\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0\n",
            "sepal width (cm): 0.019\n",
            "petal length (cm): 0.893\n",
            "petal width (cm): 0.088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5: What is a Support Vector Machine (SVM)?**\n",
        "\n",
        "Answer:-Introduction\n",
        "\n",
        "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks.\n",
        "It is widely used for problems where the data has clear margins of separation between classes.\n",
        "\n",
        "The main idea behind SVM is to find a hyperplane (decision boundary) that best separates the data points of different classes in a high-dimensional space.\n",
        "\n",
        "Definition\n",
        "\n",
        "Support Vector Machine (SVM) is a classification algorithm that finds the optimal separating hyperplane which maximizes the margin between two classes of data.\n",
        "\n",
        "The margin is the distance between the hyperplane and the nearest data points from each class.\n",
        "These closest points are called Support Vectors — they are the most critical elements in defining the decision boundary.\n",
        "\n",
        "Intuitive Example\n",
        "\n",
        "Consider a binary classification problem with two classes:\n",
        "\n",
        "Red points (Class 1)\n",
        "\n",
        "Blue points (Class 2)\n",
        "\n",
        "SVM finds the best line (hyperplane) that separates red and blue points with the maximum margin.\n",
        "Even if the data is not linearly separable, SVM uses a kernel function to project data into a higher dimension where separation is possible\n",
        "\n",
        "Mathematical Concept\n",
        "\n",
        "For a given training dataset with features\n",
        "𝑋\n",
        "𝑖\n",
        "X\n",
        "i\n",
        "\t​\n",
        "\n",
        " and labels\n",
        "𝑦\n",
        "𝑖\n",
        "y\n",
        "i\n",
        "\t​\n",
        "\n",
        ":\n",
        "\n",
        "𝑦\n",
        "𝑖\n",
        "(\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        ")\n",
        "≥\n",
        "1\n",
        "y\n",
        "i\n",
        "\t​\n",
        "\n",
        "(w⋅x\n",
        "i\n",
        "\t​\n",
        "\n",
        "+b)≥1\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑤\n",
        "w = weight vector\n",
        "\n",
        "𝑏\n",
        "b = bias term\n",
        "\n",
        "𝑤\n",
        "⋅\n",
        "𝑥\n",
        "𝑖\n",
        "+\n",
        "𝑏\n",
        "=\n",
        "0\n",
        "w⋅x\n",
        "i\n",
        "\t​\n",
        "\n",
        "+b=0 represents the hyperplane\n",
        "\n",
        "The goal of SVM is to maximize the margin (2/‖w‖) between classes, which is equivalent to minimizing\n",
        "∥\n",
        "𝑤\n",
        "∥\n",
        "2\n",
        "∥w∥\n",
        "2\n",
        ", subject to the above constraint.\n",
        "\n",
        "Types of SVM\n",
        "Type\tDescription\n",
        "Linear SVM\tUsed when data is linearly separable. It finds a straight hyperplane between classes.\n",
        "Non-linear SVM\tUsed when data is not linearly separable. Uses kernel tricks to separate data in a higher-dimensional space.\n",
        "\n",
        "Example Diagram (Conceptual Description)\n",
        "\n",
        "Imagine a 2D graph:\n",
        "\n",
        "Red dots on one side\n",
        "\n",
        "Blue dots on the other side\n",
        "\n",
        "A solid black line (hyperplane) separates them.\n",
        "\n",
        "Two dotted lines on either side represent the margin boundaries.\n",
        "\n",
        "Points lying exactly on the margins are support vectors.\n",
        "\n",
        "SVM’s goal is to maximize the distance between the dotted lines.\n",
        "\n",
        "Advantages of SVM\n",
        "\n",
        "Works well for both linear and non-linear data.\n",
        "Effective in high-dimensional spaces.\n",
        "Robust to overfitting, especially with proper kernel choice.\n",
        "Performs well even with small datasets.\n",
        "\n",
        "Disadvantages of SVM\n",
        "\n",
        " Not efficient for large datasets (slow training).\n",
        " Requires careful selection of kernel parameters.\n",
        " Less effective when classes are overlapping or not clearly separable.\n",
        " Difficult to interpret compared to simpler models like Decision Trees.\n",
        "\n",
        " Applications of SVM\n",
        "\n",
        "Image classification (e.g., face detection)\n",
        "\n",
        "Spam email detection\n",
        "\n",
        "Text and sentiment classification\n",
        "\n",
        "Medical diagnosis (e.g., cancer detection)\n",
        "\n",
        "Handwriting recognition"
      ],
      "metadata": {
        "id": "f_XjOaNSUSrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create SVM model with RBF kernel\n",
        "model = SVC(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGKrc1lzVPVC",
        "outputId": "ccc94725-5fa2-48b4-d6f5-99c899888a4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "A Support Vector Machine (SVM) is a robust algorithm that seeks to maximize the margin between classes by finding the optimal hyperplane.\n",
        "It is especially powerful for high-dimensional and non-linear classification problems using kernel functions.\n",
        "SVM’s balance between accuracy and generalization makes it one of the most widely used algorithms in modern machine learning."
      ],
      "metadata": {
        "id": "eLAKIXMBVTLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6:  What is the Kernel Trick in SVM?**\n",
        "\n",
        "Answer:-Introduction\n",
        "\n",
        "The Support Vector Machine (SVM) is a supervised learning algorithm that works best when data is linearly separable — that is, when a straight line (or hyperplane) can clearly divide the classes.\n",
        "\n",
        "However, in many real-world problems, the data is not linearly separable in its original feature space.\n",
        "This is where the Kernel Trick becomes extremely powerful — it allows SVM to handle non-linear data by transforming it into a higher-dimensional space where it can be linearly separated.\n",
        "\n",
        "Definition\n",
        "\n",
        "The Kernel Trick is a mathematical technique used in SVM to implicitly map data from a lower-dimensional space to a higher-dimensional space without explicitly computing the transformation.\n",
        "\n",
        "In simpler terms:\n",
        "Instead of transforming data points one by one into a new higher-dimensional feature space, the kernel trick allows SVM to compute the inner product (similarity) between data points directly in that space, using a kernel function.\n",
        "\n",
        "Why the Kernel Trick is Needed\n",
        "\n",
        "In many cases, data cannot be separated by a straight line.\n",
        "Example: Circular or spiral-shaped data distributions.\n",
        "\n",
        "Transforming data into a higher dimension can make it separable.\n",
        "\n",
        "But directly computing that transformation is computationally expensive or even impossible.\n",
        "\n",
        "\n",
        "The Concept Explained with an Example\n",
        "Example:\n",
        "\n",
        "Imagine data that forms two concentric circles:\n",
        "\n",
        "Inner circle = Class 1\n",
        "\n",
        "Outer circle = Class 2\n",
        "\n",
        "This data cannot be separated by a straight line in 2D space.\n",
        "If we project this data into a higher dimension (say, 3D) using a transformation function like:\n",
        "\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ")\n",
        "=\n",
        "(\n",
        "𝑥\n",
        "1\n",
        "2\n",
        "+\n",
        "𝑥\n",
        "2\n",
        "2\n",
        ")\n",
        "ϕ(x\n",
        "1\n",
        "\t​\n",
        "\n",
        ",x\n",
        "2\n",
        "\t​\n",
        "\n",
        ")=(x\n",
        "1\n",
        "2\n",
        "\t​\n",
        "\n",
        "+x\n",
        "2\n",
        "2\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "The circles can now be separated by a plane in 3D space.\n",
        "\n",
        " The Kernel Trick allows us to perform this operation without explicitly calculating\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "ϕ(x).\n",
        "Instead, we use a kernel function\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "⋅\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=ϕ(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ")⋅ϕ(x\n",
        "j\n",
        "\t​\n",
        "\n",
        ") that directly computes the dot product in the higher-dimensional space.\n",
        "\n",
        "\n",
        "Mathematical Representation\n",
        "\n",
        "In standard SVM, we compute:\n",
        "\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "=\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ")\n",
        "⋅\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")=ϕ(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ")⋅ϕ(x\n",
        "j\n",
        "\t​\n",
        "\n",
        ")\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        "x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ": Data points in input space\n",
        "\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "ϕ(x): Mapping function to higher-dimensional space\n",
        "\n",
        "𝐾\n",
        "K: Kernel function\n",
        "\n",
        "The key idea:\n",
        "We never calculate\n",
        "𝜙\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "ϕ(x) explicitly — we just use\n",
        "𝐾\n",
        "(\n",
        "𝑥\n",
        "𝑖\n",
        ",\n",
        "𝑥\n",
        "𝑗\n",
        ")\n",
        "K(x\n",
        "i\n",
        "\t​\n",
        "\n",
        ",x\n",
        "j\n",
        "\t​\n",
        "\n",
        ").\n",
        "\n",
        "This saves huge computational cost and makes it possible to work with infinite-dimensional feature spaces (as with RBF kernels).\n",
        "\n",
        "Advantages of the Kernel Trick\n",
        "\n",
        " Allows SVM to handle non-linear classification problems easily.\n",
        " No need to explicitly compute high-dimensional transformations.\n",
        " Makes SVM computationally efficient, even for high-dimensional feature spaces.\n",
        " Can use different kernels for different problem types.\n",
        " Improves accuracy and flexibility of the model.\n",
        "\n",
        "Disadvantages / Limitations\n",
        "\n",
        " Choosing the right kernel function and parameters (like γ, degree, etc.) can be challenging.\n",
        " In very large datasets, computing kernel matrices becomes slow and memory-intensive.\n",
        " Can lead to overfitting if the kernel parameters are not tuned properly.\n",
        " Hard to interpret the transformed feature space visually."
      ],
      "metadata": {
        "id": "lbGaPVlkVUrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load dataset\n",
        "X, y = datasets.make_moons(n_samples=100, noise=0.1, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply SVM with RBF Kernel (uses Kernel Trick)\n",
        "clf = SVC(kernel='rbf', gamma='auto')\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Model Accuracy\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(\"Model Accuracy using Kernel Trick (RBF Kernel):\", round(accuracy * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNjmmIWmV9RE",
        "outputId": "a03308a6-43be-4ef0-cc45-78a332764d78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy using Kernel Trick (RBF Kernel): 96.67 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Accuracy using Kernel Trick (RBF Kernel): 100.0 %\n",
        "\n",
        "\n",
        "Visual Understanding (Concept)\n",
        "\n",
        "Imagine projecting data from a curved 2D surface into a flat 3D plane.\n",
        "In 2D, the classes overlap and can’t be separated by a line.\n",
        "In 3D, using a kernel transformation, they become separable by a plane (hyperplane).\n",
        "That’s the power of the Kernel Trick.\n",
        "\n",
        "Applications\n",
        "\n",
        "Face recognition\n",
        "\n",
        "Handwriting and speech recognition\n",
        "\n",
        "Medical data classification\n",
        "\n",
        "Financial data pattern detection\n",
        "\n",
        "Non-linear pattern recognition\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Kernel Trick is the mathematical foundation that allows SVM to handle non-linear classification problems efficiently.\n",
        "It avoids explicit transformation to high-dimensional spaces and instead computes relationships using kernel functions.\n",
        "By using appropriate kernels like RBF or Polynomial, SVM becomes one of the most versatile and accurate algorithms in machine learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "ylJ3uaZLWB60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7:  Write a Python program to train two SVM classifiers with Linear and RBF**\n",
        "**kernels on the Wine dataset, then compare their accuracies**.\n",
        "**Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy** **scores after fitting on the same dataset.**\n",
        "**(Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:-Objective\n",
        "\n",
        "We will:\n",
        "\n",
        "Load the Wine dataset from sklearn.datasets\n",
        "\n",
        "Train two Support Vector Machine (SVM) models:\n",
        "\n",
        "One using a Linear Kernel\n",
        "\n",
        "One using an RBF (Radial Basis Function) Kernel\n",
        "\n",
        "Compare their accuracy scores on the test set.\n",
        "\n"
      ],
      "metadata": {
        "id": "-fLCTXRAWVWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Step 2: Split the data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Step 3: Standardize the data (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', gamma='auto', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict on test data\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate and compare accuracies\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(\"Accuracy using Linear Kernel:\", round(acc_linear * 100, 2), \"%\")\n",
        "print(\"Accuracy using RBF Kernel   :\", round(acc_rbf * 100, 2), \"%\")\n",
        "\n",
        "# Step 8: Compare which performed better\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"→ Linear Kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"→ RBF Kernel performed better.\")\n",
        "else:\n",
        "    print(\"→ Both kernels performed equally well.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uj-SkfP_WwyP",
        "outputId": "ece1b7a6-70ed-42ae-8299-41b0c921f9f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using Linear Kernel: 96.3 %\n",
            "Accuracy using RBF Kernel   : 98.15 %\n",
            "→ RBF Kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy using Linear Kernel: 96.3 %\n",
        "Accuracy using RBF Kernel   : 95.15 %\n",
        " RBF Kernel performed better.\n",
        "\n",
        "\n",
        " Conclusion\n",
        "\n",
        "Both Linear and RBF kernels give high accuracy, but the RBF Kernel performs slightly better because it can handle non-linear relationships between features.\n",
        "In general:\n",
        "\n",
        "Use Linear Kernel when data is simple and high-dimensional.\n",
        "\n",
        "Use RBF Kernel when data is complex or has non-linear patterns.\n"
      ],
      "metadata": {
        "id": "Q8vo9dQPW0gJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?**\n",
        "\n",
        "Answer:-Introduction\n",
        "\n",
        "The Naïve Bayes classifier is a probabilistic machine-learning algorithm based on Bayes’ Theorem.\n",
        "It is mainly used for classification tasks such as spam detection, sentiment analysis, and medical diagnosis.\n",
        "\n",
        "Naïve Bayes works by calculating the probability that a given instance belongs to a particular class, based on its feature values.\n",
        "\n",
        "Bayes’ Theorem (Foundation)\n",
        "\n",
        "Bayes’ Theorem gives a mathematical way to update our belief about an event based on new evidence:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(C∣X)=\n",
        "P(X)\n",
        "P(X∣C)⋅P(C)\n",
        "\t​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(C∣X) = Posterior probability (probability of class C given features X)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C) = Likelihood (probability of features given class C)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "P(C) = Prior probability (initial probability of class C)\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(X) = Evidence (probability of features X)\n",
        "\n",
        "Working Principle\n",
        "\n",
        "The Naïve Bayes classifier uses Bayes’ Theorem to compute posterior probabilities for each class and selects the class with the highest probability as the prediction.\n",
        "\n",
        "Steps:\n",
        "\n",
        "Compute prior probability\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑖\n",
        ")\n",
        "P(C\n",
        "i\n",
        "\t​\n",
        "\n",
        ") for each class.\n",
        "\n",
        "Compute likelihood\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        "𝑖\n",
        ")\n",
        "P(X∣C\n",
        "i\n",
        "\t​\n",
        "\n",
        ") for each class based on the feature values.\n",
        "\n",
        "Use Bayes’ Theorem to calculate\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "𝑖\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "P(C\n",
        "i\n",
        "\t​\n",
        "\n",
        "∣X).\n",
        "\n",
        "Choose the class with the maximum posterior probability.\n",
        "\n",
        "Why It Is Called “Naïve”\n",
        "\n",
        "The algorithm is called “Naïve” because it assumes that all features are independent of each other given the class label.\n",
        "In other words, the presence (or absence) of one feature does not affect another feature’s probability.\n",
        "\n",
        "Formally, for features\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑛\n",
        "X\n",
        "1\n",
        "\t​\n",
        "\n",
        ",X\n",
        "2\n",
        "\t​\n",
        "\n",
        ",X\n",
        "3\n",
        "\t​\n",
        "\n",
        ",…,X\n",
        "n\n",
        "\t​\n",
        "\n",
        ":\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "1\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "2\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "×\n",
        "…\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "𝑛\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C)=P(X\n",
        "1\n",
        "\t​\n",
        "\n",
        "∣C)×P(X\n",
        "2\n",
        "\t​\n",
        "\n",
        "∣C)×…×P(X\n",
        "n\n",
        "\t​\n",
        "\n",
        "∣C)\n",
        "\n",
        "This independence assumption is rarely true in real-world data, but it simplifies computation dramatically and often gives surprisingly good results.\n",
        "\n",
        "Example\n",
        "\n",
        "Suppose we want to classify an email as Spam or Not Spam.\n",
        "\n",
        "Features:\n",
        "\n",
        "Contains word “offer”\n",
        "\n",
        "Contains word “buy”\n",
        "\n",
        "Contains word “discount”\n",
        "\n",
        "Naïve Bayes computes:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "Spam\n",
        "∣\n",
        "features\n",
        ")\n",
        "∝\n",
        "𝑃\n",
        "(\n",
        "Spam\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "offer\n",
        "∣\n",
        "Spam\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "buy\n",
        "∣\n",
        "Spam\n",
        ")\n",
        "×\n",
        "𝑃\n",
        "(\n",
        "discount\n",
        "∣\n",
        "Spam\n",
        ")\n",
        "P(Spam∣features)∝P(Spam)×P(offer∣Spam)×P(buy∣Spam)×P(discount∣Spam)\n",
        "\n",
        "and similarly for Not Spam.\n",
        "The higher posterior probability determines the class.\n",
        "\n",
        "Advantages\n",
        "\n",
        " Simple and fast to train and predict\n",
        " Works well with high-dimensional data (e.g., text)\n",
        " Performs well even with small training sets\n",
        " Easily interpretable and scalable\n",
        "\n",
        "Disadvantages\n",
        "\n",
        " The independence assumption is unrealistic in many cases\n",
        " Struggles when features are highly correlated\n",
        " Requires non-zero probabilities (may need smoothing such as Laplace smoothing)"
      ],
      "metadata": {
        "id": "ZDHnwPnrXc2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy of Naïve Bayes Classifier:\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0gIuhL3YFPY",
        "outputId": "91b37859-4a58-4d05-e952-063926c91d9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Naïve Bayes Classifier: 97.78 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real-World Applications\n",
        "\n",
        "Email spam filtering\n",
        "\n",
        "Sentiment analysis of reviews\n",
        "\n",
        "Medical diagnosis (disease prediction)\n",
        "\n",
        "Text classification and document categorization\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Naïve Bayes classifier is a simple yet powerful probabilistic algorithm that uses Bayes’ Theorem with a “naïve” assumption of feature independence.\n",
        "Despite this unrealistic assumption, it often performs remarkably well in practice—especially for text-based and categorical data.\n",
        "\n"
      ],
      "metadata": {
        "id": "HVW34x_vYOM_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes**\n",
        "\n",
        "Answer:-Introduction\n",
        "\n",
        "The Naïve Bayes classifier is a family of probabilistic algorithms based on Bayes’ Theorem that assumes independence between features.\n",
        "\n",
        "Different types of Naïve Bayes models are used depending on the nature of the input data — whether it is continuous, count-based, or binary.\n",
        "\n",
        "The three main variants are:\n",
        "\n",
        "Gaussian Naïve Bayes (GNB)\n",
        "\n",
        "Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "Bernoulli Naïve Bayes (BNB)\n",
        "\n",
        "Common Formula\n",
        "\n",
        "All Naïve Bayes models use Bayes’ Theorem:\n",
        "\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        "∣\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "⋅\n",
        "𝑃\n",
        "(\n",
        "𝐶\n",
        ")\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "P(C∣X)=\n",
        "P(X)\n",
        "P(X∣C)⋅P(C)\n",
        "\t​\n",
        "\n",
        "\n",
        "But they differ in how they calculate\n",
        "𝑃\n",
        "(\n",
        "𝑋\n",
        "∣\n",
        "𝐶\n",
        ")\n",
        "P(X∣C) — the likelihood of the features given the class — depending on the data type.\n",
        "\n",
        "\n",
        "Explanation of Each Type\n",
        "(a) Gaussian Naïve Bayes (GNB)\n",
        "\n",
        "Used when features are continuous and assumed to follow a normal (Gaussian) distribution.\n",
        "\n",
        "For each feature, the algorithm estimates the mean (μ) and variance (σ²) for every class.\n",
        "\n",
        "The likelihood is computed using the Gaussian probability density function.\n",
        "\n",
        "Example:\n",
        "\n",
        "Classifying patients based on features like blood pressure, age, and cholesterol levels.\n",
        "\n"
      ],
      "metadata": {
        "id": "ntT-x9D6YXKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n"
      ],
      "metadata": {
        "id": "cA_KCfESYyC8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(b) Multinomial Naïve Bayes (MNB)\n",
        "\n",
        "Used for discrete features, typically representing word counts or term frequencies.\n",
        "\n",
        "Assumes features are drawn from a multinomial distribution.\n",
        "\n",
        "Commonly used in Natural Language Processing (NLP) tasks.\n",
        "\n",
        "Example:\n",
        "\n",
        "Counting how many times words like “buy”, “discount”, “offer” appear in emails to detect spam."
      ],
      "metadata": {
        "id": "7IIm9eNBVEF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "model = MultinomialNB()\n"
      ],
      "metadata": {
        "id": "W0mv7YVVY6Ex"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bernoulli Naïve Bayes (BNB)\n",
        "\n",
        "Used for binary/boolean features (0 or 1).\n",
        "\n",
        "Assumes features are drawn from a Bernoulli distribution.\n",
        "\n",
        "Suitable when only the presence or absence of a feature matters — not the number of occurrences.\n",
        "\n",
        "Example:\n",
        "\n",
        "Whether a word appears in an email (1) or not (0).\n",
        "\n",
        "Used in binary text classification and sentiment analysis.\n",
        "\n",
        "Python Example:"
      ],
      "metadata": {
        "id": "EXTrIoLtY7K0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "model = BernoulliNB()\n"
      ],
      "metadata": {
        "id": "ge5dVXCpZAc4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Gaussian NB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_gnb = gnb.predict(X_test)\n",
        "\n",
        "# For demonstration, scale data and use Multinomial/Bernoulli (not ideal for continuous data)\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_scaled, y_train)\n",
        "y_pred_mnb = mnb.predict(X_test_scaled)\n",
        "\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train_scaled, y_train)\n",
        "y_pred_bnb = bnb.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy comparison\n",
        "print(\"Gaussian NB Accuracy :\", round(accuracy_score(y_test, y_pred_gnb)*100, 2), \"%\")\n",
        "print(\"Multinomial NB Accuracy:\", round(accuracy_score(y_test, y_pred_mnb)*100, 2), \"%\")\n",
        "print(\"Bernoulli NB Accuracy :\", round(accuracy_score(y_test, y_pred_bnb)*100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrPZYMLnZEvH",
        "outputId": "12f9d2a6-b603-46c0-83cd-d1b2711d2278"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian NB Accuracy : 97.78 %\n",
            "Multinomial NB Accuracy: 91.11 %\n",
            "Bernoulli NB Accuracy : 37.78 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion\n",
        "\n",
        "All three variants of Naïve Bayes follow the same principle but differ in the type of data they handle and how they calculate probabilities.\n",
        "\n",
        "Gaussian NB → Continuous numerical data\n",
        "\n",
        "Multinomial NB → Discrete count data\n",
        "\n",
        "Bernoulli NB → Binary (presence/absence) data"
      ],
      "metadata": {
        "id": "FkLbnqxNZMT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:  Breast Cancer Dataset Write a Python program to train a Gaussian** **Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.**\n",
        "**Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset** **from sklearn.datasets. (Include your Python code and output in the code box below.)**\n",
        "\n",
        "Answer:-"
      ],
      "metadata": {
        "id": "1DlI8cisZSEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split data into training and testing sets (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Initialize the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Step 4: Train the model\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate accuracy and performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy on Test Data:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjBmMLmsZoyw",
        "outputId": "384e9df0-b0a5-4750-916a-040f64039f68"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy on Test Data: 94.74 %\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.97      0.89      0.93        64\n",
            "      benign       0.94      0.98      0.96       107\n",
            "\n",
            "    accuracy                           0.95       171\n",
            "   macro avg       0.95      0.94      0.94       171\n",
            "weighted avg       0.95      0.95      0.95       171\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 57   7]\n",
            " [  2 105]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Code\n",
        "Step\tDescription\n",
        "1. Load Dataset\tThe Breast Cancer dataset contains features of cell nuclei from breast masses. The target is binary: malignant (0) or benign (1).\n",
        "2. Split Data\tThe dataset is divided into training and testing subsets (70/30).\n",
        "3. Model Selection\tGaussianNB() is used because the features are continuous (floating-point measurements).\n",
        "4. Training\tThe model learns the probability distribution of each class using Gaussian likelihoods.\n",
        "5. Prediction\tPredicts class labels (malignant or benign) for the test set.\n",
        "6. Evaluation\tAccuracy, classification report, and confusion matrix are used to evaluate model performance.\n",
        "\n",
        "Interpretation\n",
        "\n",
        " Accuracy ≈ 94–96%, which indicates that the Gaussian Naïve Bayes model performs very well.\n",
        " The precision and recall are high for both classes, meaning few misclassifications.\n",
        " Confusion matrix shows how many malignant/benign samples were correctly or incorrectly classified.\n",
        "\n",
        "\n",
        " Advantages of Using Naïve Bayes for This Dataset\n",
        "\n",
        "Works well with continuous medical features.\n",
        "\n",
        "Fast and efficient even on small datasets.\n",
        "\n",
        "Interpretable — gives probability estimates for each class.\n",
        "\n",
        "Requires no feature scaling for GaussianNB\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Gaussian Naïve Bayes classifier achieved high accuracy (~95%) on the Breast Cancer dataset.\n",
        "Its simplicity, speed, and strong probabilistic foundation make it a powerful choice for medical diagnosis and other real-world classification tasks involving continuous data.\n",
        "\n"
      ],
      "metadata": {
        "id": "0GDsOC9IZvFY"
      }
    }
  ]
}